## Welcome to TruthAIOrg! üëã

<!--

**Here are some ideas to get you started:**

üôã‚Äç‚ôÄÔ∏è A short introduction - what is your organization all about?
üåà Contribution guidelines - how can the community get involved?
üë©‚Äçüíª Useful resources - where can the community find your docs? Is there anything else the community should know?
üçø Fun facts - what does your team eat for breakfast?
üßô Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->

TruthAIOrg is a community focused on generalized vision-based AI. We strive to develop models that not only excel at one vision benchmark, but can have a general understanding of vision so that little effort is needed to adapt to new vision-based tasks. We develop model architecture and release pre-trained models to the community to motivate further research in this area. We have made promising progress in terms of general vision AI, with ***57 SOTA*** rankings from our models both for image-based and video-based tasks. We hope to empower individuals and businesses by offering a higher starting point to develop vision-based AI products and lessening the burdun of building an AI model from scratch.

![WechatIMG711](https://user-images.githubusercontent.com/123792031/233248283-956dea03-7c99-4d43-8adb-33a7a3a19f6f.jpeg)

### Our Work

* ### [InternImage](https://github.com/OpenGVLab/InternImage) üëà

  Best performing image-based universal backbone model with up to 3 billion parameters
  
  90.1% Top1 accuracy in ImageNet, 65.5 mAP on COCO object detection

  > Related projects
  
  * [STM-Evaluation](https://github.com/OpenGVLab/STM-Evaluation) - A unified architecture for different spatial token mixing paradigms, and make various comparisons and analyses for these "spatial token mixers".
  * [M3I-Pretraining](https://github.com/OpenGVLab/M3I-Pretraining) - Successfully pre-train a 1B model (InternImage-H) with M3I Pre-training and achieve new record 65.4 mAP on COCO detection test-dev, 62.5 mAP on LVIS detection minival, and 62.9 mIoU on ADE20k.
  * [ConvMAE](https://github.com/OpenGVLab/Official-ConvMAE-Det) - Transfer learning for object detection on COCO.

* ### [InternVideo](https://github.com/OpenGVLab/InternVideo) üëà

  The first video foundation model to achieve high-performance on both video and video-text tasks. 
  
  SOTA performance on 39 video datasets when released in 2022.
  
  91.1% Top1 accuracy in Kinetics 400, 77.2% Top1 accuracy in Something-Something V2.
  
  > Related projects

  * [LORIS](https://github.com/OpenGVLab/LORIS) - Our model generates long-term soundtracks with state-of-the-art musical quality and rhythmic correspondence
  * üî• [Ask-Anything](https://github.com/OpenGVLab/Ask-Anything) - A simple yet interesting tool for chatting with video
  * üî• [VideoMAEv2](https://github.com/OpenGVLab/VideoMAEv2) - Successfully train a video ViT model with a billion parameters, which achieves a new SOTA performance on the datasets of Kinetics and Something-Something, and many more.
  * [Unmasked Teacher](https://github.com/OpenGVLab/unmasked_teacher) - Our scratch-built ViT-L/16 achieves SOTA performances on various video tasks.
  * [UniFormerV2](https://github.com/OpenGVLab/UniFormerV2) - The first model to achieve 90% top-1 accuracy on Kinetics-400.
  * [Efficient Video Learners](https://github.com/OpenGVLab/efficient-video-recognition) - Despite with a small training computation and memory consumption, EVL models achieves high performance on Kinetics-400.

* ### General 3D

  * üî• [HumanBench](https://github.com/OpenGVLab/HumanBench) - A Large-scale and diverse Human-centric benchmark, and many more.

* ### Competition winning solutions üèÜ

  * [InternVideo-Ego4D](https://github.com/OpenGVLab/ego4d-eccv2022-solutions) - 1st place in 5 Ego4D challenges, ECCV 2022

### Follow us

* [Twitter](https://twitter.com/opengvlab)
* [WeChat](./profile/opengv-wechat.jpeg)
* [Hugging Face](https://huggingface.co/OpenGVLab)
